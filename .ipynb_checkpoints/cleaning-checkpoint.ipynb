{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd7bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08c491b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 12)\n"
     ]
    }
   ],
   "source": [
    "dat = pd.read_csv(\"yelp_boston.csv\")\n",
    "dat = dat.dropna().reset_index()\n",
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0848d46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Financial District', 'North End', 'Waterfront', 'East Boston',\n",
       "       'Downtown', 'South End', 'Beacon Hill', 'Back Bay', 'South Boston',\n",
       "       'Chinatown', 'Allston/Brighton', 'Charlestown',\n",
       "       'Kendall Square/MIT', 'Dorchester', 'Teele Square',\n",
       "       'Jamaica Plain', 'Inman Square', 'Harvard Square', 'Fenway',\n",
       "       'Mission Hill', 'Porter Square', 'North Cambridge', 'West Roxbury',\n",
       "       'Coolidge Corner'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[\"neighborhood\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf91f2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "search category\n",
       "pizza            20\n",
       "newamerican      17\n",
       "sandwiches       17\n",
       "italian          17\n",
       "japanese         17\n",
       "restaurants      16\n",
       "mexican          16\n",
       "vietnamese       16\n",
       "bakeries         16\n",
       "chinese          16\n",
       "coffee           16\n",
       "sushi            16\n",
       "cafes            16\n",
       "indpak           12\n",
       "french           11\n",
       "thai              9\n",
       "donuts            9\n",
       "ethnicmarkets     9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dat[\"search category\"].unique()))\n",
    "dat[\"search category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49fd2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['categories_json'] = dat['categories_json'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "dat['categories_json'] = dat['categories_json'].apply(lambda lst: [item[0] for item in lst])\n",
    "max_categories = max(dat['categories_json'].apply(len))\n",
    "for i in range(max_categories):\n",
    "    dat[f'category{i+1}'] = dat['categories_json'].apply(lambda x: x[i] if i < len(x) else None)\n",
    "dat = dat.drop(columns=['categories_json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d256bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_except_category = [col for col in dat.columns if col != \"search category\"]\n",
    "new_dat = pd.DataFrame(columns=columns_except_category + [\"search category1\", \"search category2\", \"search category3\"])\n",
    "\n",
    "for n in dat[\"name\"].unique():\n",
    "    d = dat[dat[\"name\"] == n]\n",
    "    unique_categories = d[\"search category\"].unique().tolist()\n",
    "    \n",
    "    while len(unique_categories) < 3:\n",
    "        unique_categories += [None, None]\n",
    "\n",
    "    unique_categories = unique_categories[:3]\n",
    "    row = d.iloc[0][columns_except_category].tolist() + unique_categories\n",
    "    new_dat.loc[len(new_dat)] = row\n",
    "\n",
    "dat = new_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad39d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizza            19\n",
      "japanese         17\n",
      "italian          17\n",
      "newamerican      17\n",
      "sandwiches       17\n",
      "restaurants      16\n",
      "bakeries         16\n",
      "mexican          16\n",
      "chinese          16\n",
      "sushi            16\n",
      "cafes            15\n",
      "coffee           15\n",
      "vietnamese       14\n",
      "indpak           12\n",
      "french           11\n",
      "thai              9\n",
      "ethnicmarkets     9\n",
      "donuts            8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "category_counts = dat[[\"search category1\", \"search category2\", \"search category3\"]].stack().value_counts()\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7553606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.unique(dat[[\"search category1\", \"search category2\", \"search category3\"]].values.ravel())\n",
    "categories = [cat for cat in categories if pd.notna(cat)]\n",
    "# Create a dictionary to store category occurrences\n",
    "category_dict = {category: set(dat.index[dat.isin([category]).any(axis=1)]) for category in categories}\n",
    "minor_cats = []\n",
    "\n",
    "# Calculate and print overlap percentages\n",
    "for cat1, cat2 in combinations(categories, 2):\n",
    "    set1 = category_dict[cat1]\n",
    "    set2 = category_dict[cat2]\n",
    "    \n",
    "    percentage1 = len(set1 & set2) / len(set1) * 100\n",
    "    percentage2 = len(set1 & set2) / len(set2) * 100\n",
    "    if percentage1 > 50:\n",
    "        minor_cats += [cat1]\n",
    "        print(f\"Percentage of rows in {cat1} that also has {cat2}: {percentage1:.2f}%\")\n",
    "    if percentage2 > 50:\n",
    "        minor_cats += [cat2]\n",
    "        print(f\"Percentage of rows in {cat2} that also has {cat1}: {percentage2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all categories into a single list\n",
    "all_categories = pd.unique(dat[[\"search category1\", \"search category2\", \"search category3\"]].values.ravel())\n",
    "all_categories = [cat for cat in all_categories if pd.notna(cat)]  # Remove NaN values\n",
    "\n",
    "# Dictionary to store counts\n",
    "category_counts = {}\n",
    "\n",
    "# Iterate over unique categories\n",
    "for category in all_categories:\n",
    "    # Find rows where the category appears\n",
    "    mask = (dat[\"search category1\"] == category) | (dat[\"search category2\"] == category) | (dat[\"search category3\"] == category)\n",
    "    \n",
    "    # Count rows where only one category is non-null\n",
    "    single_category_count = dat[mask].dropna(subset=[\"search category2\", \"search category3\"], how='any').shape[0]\n",
    "    \n",
    "    category_counts[category] = single_category_count\n",
    "\n",
    "# Print results\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefc47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where any category column contains \"Bubble Tea\"\n",
    "bubble_tea_rows = dat[\n",
    "    dat[[\"search category1\", \"search category2\", \"search category3\"]].isin([\"sushi\"]).any(axis=1)\n",
    "]\n",
    "\n",
    "# Display the result\n",
    "print(bubble_tea_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_pivoted = dat.groupby(dat.columns.difference(['search category']).tolist()) \\\n",
    "                 .agg({'search category': list}) \\\n",
    "                 .reset_index()\n",
    "\n",
    "# Step 2: Expand search categories into multiple columns dynamically\n",
    "max_categories = dat_pivoted['search category'].apply(len).max()  # Find max categories in a row\n",
    "\n",
    "# Create new category columns dynamically\n",
    "category_cols = [f'search_category{i+1}' for i in range(max_categories)]\n",
    "dat_pivoted[category_cols] = pd.DataFrame(dat_pivoted['search category'].apply(lambda x: x + [None] * (max_categories - len(x))).tolist())\n",
    "\n",
    "# Drop the original list column\n",
    "dat_pivoted.drop(columns=['search category'], inplace=True)\n",
    "dat_pivoted[\"name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5dc653",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[dat[\"search category\"] == \"restaurants\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6752c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat[\"rating\"].min())\n",
    "print(dat[\"rating\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b8e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['categories_json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc93e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.to_json(\"bgg_dataset_cleaned.json\", orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dat[[\"category1\", \"category2\", \"category3\", \"category4\"]].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23034221",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = dat[[\"category1\", \"category2\", \"category3\", \"category4\"]].stack().value_counts()\n",
    "print(category_counts[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.unique(dat[[\"category1\", \"category2\", \"category3\", \"category4\"]].values.ravel())\n",
    "categories = [cat for cat in categories if pd.notna(cat)]\n",
    "# Create a dictionary to store category occurrences\n",
    "category_dict = {category: set(dat.index[dat.isin([category]).any(axis=1)]) for category in categories}\n",
    "minor_cats = []\n",
    "\n",
    "# Calculate and print overlap percentages\n",
    "for cat1, cat2 in combinations(categories, 2):\n",
    "    set1 = category_dict[cat1]\n",
    "    set2 = category_dict[cat2]\n",
    "    \n",
    "    percentage1 = len(set1 & set2) / len(set1) * 100\n",
    "    percentage2 = len(set1 & set2) / len(set2) * 100\n",
    "    if percentage1 > 60:\n",
    "        minor_cats += [cat1]\n",
    "        print(f\"Percentage of rows in {cat1} that also has {cat2}: {percentage1:.2f}%\")\n",
    "    if percentage2 > 60:\n",
    "        minor_cats += [cat2]\n",
    "        print(f\"Percentage of rows in {cat2} that also has {cat1}: {percentage2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407bfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_categories = category_counts[category_counts < 10].index.tolist()\n",
    "minor_cats = list(set(minor_cats + minor_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e042f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.Series(categories)[~pd.Series(categories).isin(minor_cats)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(categories)[~pd.Series(categories).isin(minor_cats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~dat[['category1', 'category2', 'category3', 'category4']].isin(minor_cats).any(axis=1)\n",
    "num_rows = mask.sum()\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbabc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where any category column contains \"Bubble Tea\"\n",
    "bubble_tea_rows = dat[\n",
    "    dat[[\"category1\", \"category2\", \"category3\", \"category4\"]].isin([\"Bubble Tea\"]).any(axis=1)\n",
    "]\n",
    "\n",
    "# Display the result\n",
    "print(bubble_tea_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d98ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[\"search category\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
